{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T02:34:36.449621Z",
     "start_time": "2020-03-16T02:34:34.724186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Lambda\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import dump, load\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T02:34:36.654172Z",
     "start_time": "2020-03-16T02:34:36.651270Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T02:35:10.213782Z",
     "start_time": "2020-03-16T02:35:08.252996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 94402\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_doc(\"data/shakespeare.txt\")\n",
    "raw_text = \"\".join(filter(lambda x: not x.isdigit(), raw_text)) \n",
    "raw_text = raw_text.lower().strip()\n",
    "raw_text = re.sub(r'(\\n\\s*)+\\n', '\\n\\n', raw_text)\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "    \n",
    "# organize into sequences of characters\n",
    "length = 40\n",
    "step = 1\n",
    "sequences = []\n",
    "for i in range(length, len(raw_text), step):\n",
    "    seq = raw_text[i-length:i+1]\n",
    "    sequences.append(seq)\n",
    "    \n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "encoded_sequences = []\n",
    "\n",
    "for s in sequences:\n",
    "    encoded_seq = [mapping[char] for char in s]\n",
    "    encoded_sequences.append(encoded_seq)\n",
    "\n",
    "es = np.array(encoded_sequences)\n",
    "X, y = es[:,:-1], es[:,-1]\n",
    "es = [to_categorical(x, num_classes=len(chars)) for x in X]\n",
    "X = np.array(es)\n",
    "y = to_categorical(y, num_classes=len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RNN-LSTM and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T05:00:49.070557Z",
     "start_time": "2020-03-16T02:36:05.219027Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0315 19:36:05.221344 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0315 19:36:05.233913 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0315 19:36:05.235983 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0315 19:36:05.408257 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0315 19:36:05.424273 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0315 19:36:05.608199 4567434688 deprecation.py:323] From /Users/jma/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0315 19:36:06.040989 4567434688 deprecation_wrapper.py:119] From /Users/jma/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 88s - loss: 2.2585 - acc: 0.3449\n",
      "Epoch 2/100\n",
      " - 83s - loss: 1.8342 - acc: 0.4501\n",
      "Epoch 3/100\n",
      " - 84s - loss: 1.6844 - acc: 0.4874\n",
      "Epoch 4/100\n",
      " - 83s - loss: 1.5895 - acc: 0.5119\n",
      "Epoch 5/100\n",
      " - 81s - loss: 1.5125 - acc: 0.5320\n",
      "Epoch 6/100\n",
      " - 81s - loss: 1.4471 - acc: 0.5501\n",
      "Epoch 7/100\n",
      " - 81s - loss: 1.3865 - acc: 0.5663\n",
      "Epoch 8/100\n",
      " - 81s - loss: 1.3264 - acc: 0.5837\n",
      "Epoch 9/100\n",
      " - 81s - loss: 1.2698 - acc: 0.6000\n",
      "Epoch 10/100\n",
      " - 92s - loss: 1.2168 - acc: 0.6154\n",
      "Epoch 11/100\n",
      " - 89s - loss: 1.1638 - acc: 0.6308\n",
      "Epoch 12/100\n",
      " - 91s - loss: 1.1166 - acc: 0.6454\n",
      "Epoch 13/100\n",
      " - 93s - loss: 1.0702 - acc: 0.6597\n",
      "Epoch 14/100\n",
      " - 92s - loss: 1.0313 - acc: 0.6718\n",
      "Epoch 15/100\n",
      " - 95s - loss: 0.9939 - acc: 0.6831\n",
      "Epoch 16/100\n",
      " - 95s - loss: 0.9616 - acc: 0.6922\n",
      "Epoch 17/100\n",
      " - 90s - loss: 0.9306 - acc: 0.7018\n",
      "Epoch 18/100\n",
      " - 91s - loss: 0.9007 - acc: 0.7114\n",
      "Epoch 19/100\n",
      " - 93s - loss: 0.8783 - acc: 0.7185\n",
      "Epoch 20/100\n",
      " - 89s - loss: 0.8555 - acc: 0.7249\n",
      "Epoch 21/100\n",
      " - 90s - loss: 0.8347 - acc: 0.7309\n",
      "Epoch 22/100\n",
      " - 91s - loss: 0.8139 - acc: 0.7366\n",
      "Epoch 23/100\n",
      " - 88s - loss: 0.7983 - acc: 0.7425\n",
      "Epoch 24/100\n",
      " - 91s - loss: 0.7850 - acc: 0.7462\n",
      "Epoch 25/100\n",
      " - 90s - loss: 0.7695 - acc: 0.7504\n",
      "Epoch 26/100\n",
      " - 87s - loss: 0.7544 - acc: 0.7550\n",
      "Epoch 27/100\n",
      " - 87s - loss: 0.7430 - acc: 0.7582\n",
      "Epoch 28/100\n",
      " - 80s - loss: 0.7306 - acc: 0.7617\n",
      "Epoch 29/100\n",
      " - 84s - loss: 0.7220 - acc: 0.7633\n",
      "Epoch 30/100\n",
      " - 83s - loss: 0.7072 - acc: 0.7686\n",
      "Epoch 31/100\n",
      " - 91s - loss: 0.7006 - acc: 0.7704\n",
      "Epoch 32/100\n",
      " - 96s - loss: 0.6905 - acc: 0.7728\n",
      "Epoch 33/100\n",
      " - 87s - loss: 0.6812 - acc: 0.7761\n",
      "Epoch 34/100\n",
      " - 88s - loss: 0.6758 - acc: 0.7775\n",
      "Epoch 35/100\n",
      " - 93s - loss: 0.6595 - acc: 0.7840\n",
      "Epoch 36/100\n",
      " - 94s - loss: 0.6549 - acc: 0.7836\n",
      "Epoch 37/100\n",
      " - 90s - loss: 0.6467 - acc: 0.7854\n",
      "Epoch 38/100\n",
      " - 87s - loss: 0.6379 - acc: 0.7891\n",
      "Epoch 39/100\n",
      " - 85s - loss: 0.6331 - acc: 0.7904\n",
      "Epoch 40/100\n",
      " - 88s - loss: 0.6236 - acc: 0.7928\n",
      "Epoch 41/100\n",
      " - 85s - loss: 0.6165 - acc: 0.7966\n",
      "Epoch 42/100\n",
      " - 83s - loss: 0.6090 - acc: 0.7977\n",
      "Epoch 43/100\n",
      " - 83s - loss: 0.6042 - acc: 0.7993\n",
      "Epoch 44/100\n",
      " - 83s - loss: 0.5971 - acc: 0.8013\n",
      "Epoch 45/100\n",
      " - 84s - loss: 0.5914 - acc: 0.8022\n",
      "Epoch 46/100\n",
      " - 85s - loss: 0.5852 - acc: 0.8043\n",
      "Epoch 47/100\n",
      " - 85s - loss: 0.5776 - acc: 0.8084\n",
      "Epoch 48/100\n",
      " - 85s - loss: 0.5751 - acc: 0.8061\n",
      "Epoch 49/100\n",
      " - 84s - loss: 0.5633 - acc: 0.8115\n",
      "Epoch 50/100\n",
      " - 80s - loss: 0.5606 - acc: 0.8149\n",
      "Epoch 51/100\n",
      " - 91s - loss: 0.5573 - acc: 0.8135\n",
      "Epoch 52/100\n",
      " - 95s - loss: 0.5540 - acc: 0.8134\n",
      "Epoch 53/100\n",
      " - 90s - loss: 0.5510 - acc: 0.8140\n",
      "Epoch 54/100\n",
      " - 81s - loss: 0.5424 - acc: 0.8169\n",
      "Epoch 55/100\n",
      " - 80s - loss: 0.5380 - acc: 0.8204\n",
      "Epoch 56/100\n",
      " - 87s - loss: 0.5314 - acc: 0.8208\n",
      "Epoch 57/100\n",
      " - 96s - loss: 0.5206 - acc: 0.8252\n",
      "Epoch 58/100\n",
      " - 88s - loss: 0.5223 - acc: 0.8248\n",
      "Epoch 59/100\n",
      " - 83s - loss: 0.5196 - acc: 0.8248\n",
      "Epoch 60/100\n",
      " - 82s - loss: 0.5102 - acc: 0.8285\n",
      "Epoch 61/100\n",
      " - 84s - loss: 0.5100 - acc: 0.8261\n",
      "Epoch 62/100\n",
      " - 83s - loss: 0.5038 - acc: 0.8305\n",
      "Epoch 63/100\n",
      " - 88s - loss: 0.5001 - acc: 0.8304\n",
      "Epoch 64/100\n",
      " - 92s - loss: 0.4899 - acc: 0.8349\n",
      "Epoch 65/100\n",
      " - 83s - loss: 0.4918 - acc: 0.8339\n",
      "Epoch 66/100\n",
      " - 81s - loss: 0.4895 - acc: 0.8340\n",
      "Epoch 67/100\n",
      " - 82s - loss: 0.4779 - acc: 0.8380\n",
      "Epoch 68/100\n",
      " - 96s - loss: 0.4802 - acc: 0.8367\n",
      "Epoch 69/100\n",
      " - 84s - loss: 0.4760 - acc: 0.8386\n",
      "Epoch 70/100\n",
      " - 85s - loss: 0.4727 - acc: 0.8408\n",
      "Epoch 71/100\n",
      " - 87s - loss: 0.4683 - acc: 0.8406\n",
      "Epoch 72/100\n",
      " - 92s - loss: 0.4670 - acc: 0.8411\n",
      "Epoch 73/100\n",
      " - 90s - loss: 0.4604 - acc: 0.8438\n",
      "Epoch 74/100\n",
      " - 87s - loss: 0.4598 - acc: 0.8445\n",
      "Epoch 75/100\n",
      " - 86s - loss: 0.4569 - acc: 0.8449\n",
      "Epoch 76/100\n",
      " - 85s - loss: 0.4640 - acc: 0.8418\n",
      "Epoch 77/100\n",
      " - 88s - loss: 0.4491 - acc: 0.8464\n",
      "Epoch 78/100\n",
      " - 88s - loss: 0.4515 - acc: 0.8460\n",
      "Epoch 79/100\n",
      " - 86s - loss: 0.4457 - acc: 0.8477\n",
      "Epoch 80/100\n",
      " - 86s - loss: 0.4368 - acc: 0.8512\n",
      "Epoch 81/100\n",
      " - 89s - loss: 0.4419 - acc: 0.8483\n",
      "Epoch 82/100\n",
      " - 85s - loss: 0.4372 - acc: 0.8507\n",
      "Epoch 83/100\n",
      " - 86s - loss: 0.4383 - acc: 0.8498\n",
      "Epoch 84/100\n",
      " - 83s - loss: 0.4223 - acc: 0.8563\n",
      "Epoch 85/100\n",
      " - 84s - loss: 0.4246 - acc: 0.8550\n",
      "Epoch 86/100\n",
      " - 88s - loss: 0.4193 - acc: 0.8573\n",
      "Epoch 87/100\n",
      " - 86s - loss: 0.4217 - acc: 0.8566\n",
      "Epoch 88/100\n",
      " - 88s - loss: 0.4213 - acc: 0.8557\n",
      "Epoch 89/100\n",
      " - 88s - loss: 0.4119 - acc: 0.8594\n",
      "Epoch 90/100\n",
      " - 88s - loss: 0.4257 - acc: 0.8545\n",
      "Epoch 91/100\n",
      " - 85s - loss: 0.4144 - acc: 0.8581\n",
      "Epoch 92/100\n",
      " - 86s - loss: 0.4021 - acc: 0.8632\n",
      "Epoch 93/100\n",
      " - 84s - loss: 0.4110 - acc: 0.8595\n",
      "Epoch 94/100\n",
      " - 86s - loss: 0.4181 - acc: 0.8571\n",
      "Epoch 95/100\n",
      " - 84s - loss: 0.4196 - acc: 0.8577\n",
      "Epoch 96/100\n",
      " - 83s - loss: 0.3972 - acc: 0.8643\n",
      "Epoch 97/100\n",
      " - 84s - loss: 0.4011 - acc: 0.8634\n",
      "Epoch 98/100\n",
      " - 84s - loss: 0.4023 - acc: 0.8627\n",
      "Epoch 99/100\n",
      " - 85s - loss: 0.3922 - acc: 0.8665\n",
      "Epoch 100/100\n",
      " - 84s - loss: 0.3995 - acc: 0.8630\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n",
    " \n",
    "# save the model to file\n",
    "model.save('rnn_model/lstm_shakespeare.h5')\n",
    "# save the mapping\n",
    "dump(mapping, open('rnn_model/shakespeare_mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T05:57:46.574248Z",
     "start_time": "2020-03-16T05:57:46.567446Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "        encoded = encoded.reshape(1, encoded.shape[1], encoded.shape[2])\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-16T05:59:31.356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Sonnet Generation at Temperature = 1.5\n",
      "=========================================================\n",
      "shall i compare thee to a summer's day?\n",
      "thou art to me than my borning old me;\n",
      "nor blied of such time's life hath pleas,\n",
      "to make me give then my thoughts more doth live,\n",
      "that i am seem long secalled from love\n",
      "after my dead see thee i agains my heart:\n",
      "another till in his midder from their rest,\n",
      "being franterfur vice with too much deeper,\n",
      "  in sleep and thine ow erriming lies,\n",
      "the canker blooms for woods i loved his own,\n",
      "for no false eleminared of when i praise.\n",
      "\n",
      "without thy changing chy complexion dind.\n",
      "for whose will in these all the rearors.\n",
      "for if thou wilt, if ever sye can let me,\n",
      "which in the bressed with mire transe so die!\n",
      "  but what's so breast-to me, i see the world i sin,\n",
      "that in my brow, in wind, nor ston and bring forthing most,\n",
      "but by soir birds with thee, and they so lie:\n",
      "then can i draw a bearing of their bight,\n",
      "i see deaths infleirar end your advise.\n",
      "\n",
      "why so large of many a houry of love,\n",
      "they are but dick-latch she lies of thee,\n",
      "the earth can yet heir being dark as thee,\n",
      "the painter motion of that weile good lays,\n",
      "and beauty should i smole thee more is be,\n",
      "as this speechar longer turns art there,\n",
      "thou giv'st thou wouth (all kinder from thy brain,\n",
      "to make will know, and with the time to come\n",
      "if it with bet\n",
      "=========================================================\n",
      "RNN Sonnet Generation at Temperature = 0.75\n",
      "=========================================================\n",
      "shall i compare thee to a summer's day?\n",
      "thou art to me than my borning old me.\n",
      "\n",
      "le verswains trifm and ill, that tender spitst\n",
      "un every blood,\n",
      "  to hend more wele blood when thou wilt be decay.\n",
      "  but in the breason call of merit live.\n",
      "  for i wound on you for when thou wilt long.\n",
      "\n",
      "when give me well-kness so suffer are so skill,\n",
      "that heaven's arr yet besies an eye dowh heart,\n",
      "and that my every patient of my spent,\n",
      "time with the time to shope that thou forgel.\n",
      "\n",
      "o fresh behold and love, for the stare,\n",
      "but those was this sight with givis of spetthous night,\n",
      "and faisence someres deathers, nor end find,\n",
      "on better dumb that time do i not glance,\n",
      "and you shall sensufe the receint i not smend.\n",
      "\n",
      "that to the ending of my and incens.\n",
      "  for i impair not beauty being for my sight,\n",
      "the world in looks of bath desired every,\n",
      "nor tends thou taknest from thy self alow,\n",
      "  but when in the eye but a mistal wardy:\n",
      "  then than a cold thoughts, so true assure affece.\n",
      "\n",
      "were it not so thou thy self deservent,\n",
      "which should that hight proved that tion suppression dyed.\n",
      "the of i mad your self too grace king,\n",
      "  then that hop could so painter but and morn,\n",
      "the world will be thy widow and so it mine eyes,\n",
      "but yet like her own sweet fulber bright,\n",
      "i see \n",
      "=========================================================\n",
      "RNN Sonnet Generation at Temperature = 0.25\n",
      "=========================================================\n",
      "shall i compare thee to a summer's day?\n",
      "thou art to me than my borning old me.\n",
      "\n",
      "le verswains trift my parsless be,\n",
      "  i see the well and thress of all his briet,\n",
      "and by and by thing the world even so great,\n",
      "and purpose no nature can speak do witnoned\n",
      "the rest my self-subed find do please her treasure!\n",
      "  her to this be other part i cand i constant\n",
      "his place with that which give any spend\n",
      "the write, and the prescruses guinted name,\n",
      "sulst how thy excelles firn to the sust?\n",
      "even to the ewern of all that grows sight\n",
      "so i fect thou mayst must be thy self grow'st.\n",
      "if all thine once confound, and true,\n",
      "but like a will in looks out thine eye,\n",
      "and you in thes through of earth then her?\n",
      "and yet this with thee should you frame hade decease\n",
      "the rark to much disgrace have sweet leave me youth\n",
      "which is so pawards to the wish in me asward,\n",
      "that then beging at me in that i conque!\n",
      "  but wear this with those bolds and vice,\n",
      "crours the will in luids my gross bright.\n",
      "  where contle now on others in proving this,\n",
      "that in my xhess other lives which in write,\n",
      "a those and pretying the present-fore,\n",
      "which art a thong offence, of her vill come\n",
      "doth parst to bragh assance to the versuse,\n",
      "when you have bred ngen thee oncelexion dwell,\n",
      "when you happy\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "model = load_model('rnn_model/lstm_shakespeare.h5')\n",
    "mapping = load(open('rnn_model/shakespeare_mapping.pkl', 'rb'))\n",
    "temp_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "for temp in [1.5, 0.75, 0.25]:\n",
    "    print(\"RNN Sonnet Generation at Temperature =\", temp)\n",
    "    print(\"=========================================================\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Lambda(lambda x: x / temp))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.layers[0].set_weights(temp_weights[0])\n",
    "    model.layers[2].set_weights(temp_weights[1])\n",
    "\n",
    "    print(generate_seq(model, mapping, 40, \"shall i compare thee to a summer's day?\\n\", 1200))\n",
    "    print(\"=========================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
